{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entries = pd.read_csv(\"processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_compiled = \"\"\n",
    "\n",
    "\n",
    "#full_date,date,weekday,time,mood,activities,note_title,note\n",
    "\n",
    "for idx, entry in entries.iterrows():\n",
    "    notes_compiled += f\"\"\"\n",
    "    { str(entry['note']) if str(entry['note']) != \"nan\" else \"\"}\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "notes_compiled = notes_compiled.replace(\"<br>\", \".\")\n",
    "notes_compiled = notes_compiled.replace(\"</br>\", \".\")\n",
    "\n",
    "notes_compiled = notes_compiled.replace(\"<b>\", \".\")\n",
    "notes_compiled = notes_compiled.replace(\"</b>\", \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(notes_compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_text(notes_compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"who am i as a person and what do i value?\"\n",
    "docs = vectorstore.similarity_search(question, k=25)\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response_message = model.invoke(\n",
    "    \"my name is ara\"\n",
    ")\n",
    "\n",
    "# Compile the output to markdown\n",
    "markdown_output =response_message.content\n",
    "\n",
    "# Print or display the markdown output\n",
    "display(Markdown(markdown_output))\n",
    "\n",
    "response_message2 = model.invoke(\n",
    "    \"what is my name\"\n",
    ")\n",
    "\n",
    "# Compile the output to markdown\n",
    "markdown_output2 =response_message.content\n",
    "\n",
    "# Print or display the markdown output\n",
    "display(Markdown(markdown_output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles the memory of the conversation history\n",
    "class ChatMemory:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        \n",
    "    #Add a new exchange (user question and assistant answer) to memory.\n",
    "    def add_to_memory(self, user_input, assistant_output):\n",
    "        self.history.append(f\"User: {user_input}\")\n",
    "        self.history.append(f\"Assistant: {assistant_output}\")\n",
    "        \n",
    "    def get_context(self):\n",
    "        # Return the full context (conversation history) as a single string\n",
    "        return \"\\n\".join(self.history)\n",
    "    \n",
    "    def clear_context(self):\n",
    "        self.history = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant who keeps track of all past exchanges. Here is your conversation history\n",
    "<Conversation History>\n",
    "{history}\n",
    "</Conversation History>\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. These are some entries at different dates that give insight into your state of mind, memories and moods. Use direct references when necessary\n",
    "Be as specific and thorough as possible and prioritize conversation history when possible to be able to gain more context and quote the conversation history context when applicable\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chat_memory = ChatMemory()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    # | RunnableLambda(lambda formatted_prompt: print(f\"Formatted Prompt:\\n{formatted_prompt}\") or formatted_prompt)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    context = chat_memory.get_context()\n",
    "    \n",
    "    # Get documents related to the current question\n",
    "    docs = vectorstore.similarity_search(question, k=25)\n",
    "    \n",
    "    markdown_output = chain.invoke({\"history\": context, \"context\": docs, \"question\": question})\n",
    "    \n",
    "    # Add to the chat memory\n",
    "    chat_memory.add_to_memory(question, markdown_output)\n",
    "    \n",
    "    # Return the response\n",
    "    return markdown_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_memory.clear_context()\n",
    "\n",
    "print(\"----------------\")\n",
    "print(chat_memory.get_context())\n",
    "print(\"----------------\")\n",
    "\n",
    "question = \"have i lost some of my hobbies over time?\"\n",
    "response = ask_question(question)\n",
    "\n",
    "display(Markdown(response))\n",
    "\n",
    "# print(\"----------------\")\n",
    "# print(chat_memory.get_context())\n",
    "# print(\"----------------\")\n",
    "\n",
    "# follow_up_question = \"can you tell me more about my concerns with him?\"\n",
    "\n",
    "# follow_up_response = ask_question(follow_up_question)\n",
    "# display(Markdown(follow_up_response))\n",
    "\n",
    "# print(\"----------------\")\n",
    "# print(chat_memory.get_context())\n",
    "# print(\"----------------\")\n",
    "\n",
    "\n",
    "# # You can continue the conversation with more questions\n",
    "# another_question = \"How can I work on these feelings to be closer with him?\"\n",
    "# another_response = ask_question(another_question)\n",
    "# display(Markdown(another_response))\n",
    "\n",
    "# print(\"----------------\")\n",
    "# print(chat_memory.get_context())\n",
    "# print(\"----------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"----------------\")\n",
    "print(chat_memory.get_context())\n",
    "print(\"----------------\")\n",
    "\n",
    "question = \"how can i work to get these hobbies back?\"\n",
    "response = ask_question(question)\n",
    "\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
