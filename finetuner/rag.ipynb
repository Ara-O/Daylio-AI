{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "entries = pd.read_csv(\"processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_compiled = \"\"\n",
    "\n",
    "\n",
    "#full_date,date,weekday,time,mood,activities,note_title,note\n",
    "\n",
    "for idx, entry in entries.iterrows():\n",
    "    notes_compiled += f\"\"\"\n",
    "    { str(entry['note']) if str(entry['note']) != \"nan\" else \"\"}\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_compiled = notes_compiled.replace(\"<br>\", \".\")\n",
    "notes_compiled = notes_compiled.replace(\"</br>\", \".\")\n",
    "\n",
    "notes_compiled = notes_compiled.replace(\"<b>\", \".\")\n",
    "notes_compiled = notes_compiled.replace(\"</b>\", \".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output.txt\", \"w\") as text_file:\n",
    "    text_file.write(notes_compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_text(notes_compiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4622"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "page_content='We are currently in quiet time and I wanted to use this opportunity to reflect on who I am and the things I struggle with...My name is Eyiaraoluwa oladipo and I am 19 years old. ..My values ..I am finding it hard to pinpoint my values but part of me is also wondering whether this is the best use of my time, but in order to have fruitful relationships with other people, I need to have and know my values, and it can help me understand better why I act the way I do..One thing I value is dependability, and'\n"
     ]
    }
   ],
   "source": [
    "question = \"who am i as a person and what do i value?\"\n",
    "docs = vectorstore.similarity_search(question, k=25)\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is 1 \"R\" in the word \"strawberry\"."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "response_message = model.invoke(\n",
    "    \"how many letter rs are in strawberry\"\n",
    ")\n",
    "\n",
    "# Compile the output to markdown\n",
    "markdown_output =response_message.content\n",
    "\n",
    "# Print or display the markdown output\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatMemory:\n",
    "    \"\"\"Handles the memory of the conversation history.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        \n",
    "    def add_to_memory(self, user_input, assistant_output):\n",
    "        \"\"\"Add a new exchange (user question and assistant answer) to memory.\"\"\"\n",
    "        self.history.append(f\"User: {user_input}\")\n",
    "        self.history.append(f\"Assistant: {assistant_output}\")\n",
    "        \n",
    "    def get_context(self):\n",
    "        \"\"\"Return the full context (conversation history) as a single string.\"\"\"\n",
    "        return \"\\n\".join(self.history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are a helpful assistant who keeps track of all past exchanges. Here is your conversation history\n",
    "<Conversation History>\n",
    "{history}\n",
    "</Conversation History>\n",
    "\n",
    "Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. These are some entries at different dates that give insight into your state of mind, memories and moods. Use direct references when necessary\n",
    "Be as specific and thorough as possible\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chat_memory = ChatMemory()\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, previous_docs=None):\n",
    "    # Update this to use all context?\n",
    "    context = chat_memory.get_context() if previous_docs is None else previous_docs\n",
    "    \n",
    "    # Get documents related to the current question\n",
    "    docs = vectorstore.similarity_search(question, k=25)\n",
    "    \n",
    "    markdown_output = chain.invoke({\"history\": context, \"context\": docs, \"question\": question})\n",
    "    \n",
    "    # Add to the chat memory\n",
    "    chat_memory.add_to_memory(question, markdown_output)\n",
    "    \n",
    "    # Return the response\n",
    "    return markdown_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about my relationship with myself?\"\n",
    "response = ask_question(question)\n",
    "\n",
    "display(Markdown(response))\n",
    "\n",
    "print(chat_memory.get_context())\n",
    "\n",
    "follow_up_question = \"can you tell me more about my concerns?\"\n",
    "\n",
    "follow_up_response = ask_question(follow_up_question, previous_docs=response)\n",
    "display(Markdown(follow_up_response))\n",
    "\n",
    "print(chat_memory.get_context())\n",
    "\n",
    "\n",
    "# You can continue the conversation with more questions\n",
    "another_question = \"How can I work on these feelings to be closer with myself?\"\n",
    "another_response = ask_question(another_question, previous_docs=follow_up_response)\n",
    "display(Markdown(another_response))\n",
    "\n",
    "print(chat_memory.get_context())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
